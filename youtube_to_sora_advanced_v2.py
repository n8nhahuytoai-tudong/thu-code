#!/usr/bin/env python3

"""
YouTube Video Analyzer to Sora 2 Prompt Generator - ADVANCED VERSION 2.1
Ph√¢n t√≠ch video YouTube c·ª±c k·ª≥ chi ti·∫øt v√† t·∫°o prompts chuy√™n nghi·ªáp cho Sora 2

Features:
- Kh√¥ng gi·ªõi h·∫°n s·ªë scenes
- Ph√¢n t√≠ch chi ti·∫øt nh√¢n v·∫≠t: chi·ªÅu cao, c√¢n n·∫∑ng, m√†u da, t√≥c, trang ph·ª•c, t·ª∑ l·ªá c∆° th·ªÉ
- Ph√¢n t√≠ch chi ti·∫øt con v·∫≠t: lo√†i, k√≠ch th∆∞·ªõc, m√†u s·∫Øc, ƒë·∫∑c ƒëi·ªÉm, t·ª∑ l·ªá
- Scene detection t·ª± ƒë·ªông th√¥ng minh
- Audio transcription v·ªõi Whisper
- Visual composition analysis
- Camera movement detection
- Multiple prompt variants
- Intelligent caching
"""

import os
import sys
import time
import subprocess
import cv2
import base64
import numpy as np
from pathlib import Path
import json
from openai import OpenAI
from datetime import datetime
import hashlib
from typing import List, Dict, Optional, Any
import shutil

# ========== CONFIGURATION ==========

class Config:
    """C·∫•u h√¨nh to√†n c·ª•c"""
    # Scene detection
    SCENE_THRESHOLD = 30.0
    MIN_SCENE_LENGTH = 15
    FRAMES_PER_SCENE = 4  # TƒÉng t·ª´ 3 l√™n 4 ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt h∆°n

    # Video download
    MAX_VIDEO_HEIGHT = 1080

    # API settings
    VISION_MODEL = "gpt-4o"
    TEXT_MODEL = "gpt-4o"
    WHISPER_MODEL = "whisper-1"
    MAX_RETRIES = 3
    RETRY_DELAY = 2

    # Folders
    CACHE_DIR = "cache"
    OUTPUT_DIR = "output_results"
    TEMP_FRAMES_DIR = "temp_frames"

    # Limits - B·ªé GI·ªöI H·∫†N!
    MAX_SCENES_TO_ANALYZE = 999  # TƒÉng t·ª´ 20 l√™n 999
    MAX_SCENE_SUMMARY_LENGTH = 300  # TƒÉng t·ª´ 200 l√™n 300


# ========== UTILITIES ==========

def load_env_file():
    """Load environment variables t·ª´ file .env"""
    env_file = Path('.env')
    if env_file.exists():
        try:
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        key = key.strip()
                        value = value.strip()
                        # Remove quotes if present
                        if value.startswith('"') and value.endswith('"'):
                            value = value[1:-1]
                        if value.startswith("'") and value.endswith("'"):
                            value = value[1:-1]
                        os.environ[key] = value
        except Exception as e:
            print(f"‚ö† L·ªói ƒë·ªçc file .env: {e}")


def ensure_directories():
    """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt"""
    for dir_name in [Config.CACHE_DIR, Config.OUTPUT_DIR, Config.TEMP_FRAMES_DIR]:
        Path(dir_name).mkdir(exist_ok=True)


def cleanup_temp_files():
    """X√≥a t·∫•t c·∫£ files t·∫°m"""
    temp_files = ["temp_video.mp4", "temp_audio.m4a"]
    for f in temp_files:
        if os.path.exists(f):
            try:
                os.remove(f)
            except:
                pass

    # X√≥a temp frames
    if os.path.exists(Config.TEMP_FRAMES_DIR):
        try:
            shutil.rmtree(Config.TEMP_FRAMES_DIR)
            Path(Config.TEMP_FRAMES_DIR).mkdir(exist_ok=True)
        except:
            pass


def print_header(text: str):
    """In header ƒë·∫πp"""
    print("\n" + "="*70)
    print(text.center(70))
    print("="*70 + "\n")


def print_section(text: str):
    """In section header"""
    print("\n" + "-"*70)
    print(text)
    print("-"*70)


def print_progress(message: str, step: Optional[int] = None, total: Optional[int] = None):
    """In progress message"""
    if step and total:
        print(f"[{step}/{total}] {message}")
    else:
        print(f"‚Ä¢ {message}")


def print_success(message: str):
    """In success message"""
    print(f"‚úì {message}")


def print_error(message: str):
    """In error message"""
    print(f"‚úó {message}")


def print_warning(message: str):
    """In warning message"""
    print(f"‚ö† {message}")


# ========== MAIN CLASS ==========

class YouTubeToSoraAnalyzer:
    """
    Advanced YouTube video analyzer cho Sora 2 prompt generation
    Version 2.1 - Ph√¢n t√≠ch chi ti·∫øt nh√¢n v·∫≠t v√† con v·∫≠t
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        Kh·ªüi t·∫°o analyzer

        Args:
            api_key: OpenAI API key (optional, c√≥ th·ªÉ load t·ª´ env)
        """
        # Load .env first
        load_env_file()

        # Setup API key
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "OpenAI API key kh√¥ng t√¨m th·∫•y!\n"
                "Vui l√≤ng:\n"
                "1. T·∫°o file .env v·ªõi: OPENAI_API_KEY=sk-your-key\n"
                "2. Ho·∫∑c set bi·∫øn m√¥i tr∆∞·ªùng: export OPENAI_API_KEY=sk-your-key\n"
                "3. Ho·∫∑c truy·ªÅn api_key v√†o constructor"
            )

        # Initialize OpenAI client
        try:
            self.client = OpenAI(api_key=api_key)
        except Exception as e:
            raise ValueError(f"Kh√¥ng th·ªÉ kh·ªüi t·∫°o OpenAI client: {e}")

        # Setup directories
        ensure_directories()

        # State
        self.video_path: Optional[str] = None
        self.audio_path: Optional[str] = None
        self.youtube_url: Optional[str] = None
        self.video_title: str = ""
        self.video_metadata: Dict[str, Any] = {}
        self.scenes: List[Dict] = []
        self.frames: List[str] = []

    # ========== CACHE METHODS ==========

    def _get_cache_key(self, url: str) -> str:
        """T·∫°o cache key t·ª´ URL"""
        return hashlib.md5(url.encode()).hexdigest()

    def _save_cache(self, key: str, data: Dict):
        """L∆∞u data v√†o cache"""
        try:
            cache_file = Path(Config.CACHE_DIR) / f"{key}.json"
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print_warning(f"Kh√¥ng th·ªÉ l∆∞u cache: {e}")

    def _load_cache(self, key: str) -> Optional[Dict]:
        """Load data t·ª´ cache"""
        try:
            cache_file = Path(Config.CACHE_DIR) / f"{key}.json"
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print_warning(f"Kh√¥ng th·ªÉ ƒë·ªçc cache: {e}")
        return None

    # ========== VIDEO DOWNLOAD ==========

    def _get_video_metadata(self, youtube_url: str) -> Dict:
        """L·∫•y metadata t·ª´ YouTube"""
        print_progress("ƒêang l·∫•y th√¥ng tin video...")

        try:
            result = subprocess.run(
                ["yt-dlp", "--dump-json", youtube_url],
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode == 0:
                metadata = json.loads(result.stdout)
                self.video_title = metadata.get('title', 'Unknown')
                self.video_metadata = {
                    'title': metadata.get('title'),
                    'duration': metadata.get('duration', 0),
                    'description': (metadata.get('description') or '')[:500],
                    'uploader': metadata.get('uploader'),
                    'width': metadata.get('width'),
                    'height': metadata.get('height'),
                    'fps': metadata.get('fps'),
                }
                print_success(f"Video: {self.video_title}")
                print_success(f"Th·ªùi l∆∞·ª£ng: {self.video_metadata['duration']}s")
                return self.video_metadata
        except subprocess.TimeoutExpired:
            print_error("Timeout khi l·∫•y metadata")
        except Exception as e:
            print_error(f"L·ªói l·∫•y metadata: {e}")

        # Fallback
        self.video_title = f"Video_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return {}

    def _download_video(self, youtube_url: str) -> bool:
        """Download video v√† audio"""
        print_progress("ƒêang t·∫£i video...")

        try:
            video_output = "temp_video.mp4"
            audio_output = "temp_audio.m4a"

            # Download video
            result = subprocess.run(
                [
                    "yt-dlp",
                    "-f", f"best[height<={Config.MAX_VIDEO_HEIGHT}][ext=mp4]",
                    "-o", video_output,
                    youtube_url
                ],
                capture_output=True,
                text=True,
                timeout=300
            )

            if result.returncode != 0:
                print_error(f"L·ªói t·∫£i video: {result.stderr}")
                return False

            self.video_path = video_output
            print_success("Video ƒë√£ t·∫£i xong")

            # Download audio (optional, for transcript)
            try:
                subprocess.run(
                    ["yt-dlp", "-f", "bestaudio[ext=m4a]", "-o", audio_output, youtube_url],
                    capture_output=True,
                    timeout=60
                )
                if os.path.exists(audio_output):
                    self.audio_path = audio_output
                    print_success("Audio ƒë√£ t·∫£i xong")
            except:
                print_warning("Kh√¥ng th·ªÉ t·∫£i audio (b·ªè qua)")

            return True

        except subprocess.TimeoutExpired:
            print_error("Timeout khi t·∫£i video")
            return False
        except Exception as e:
            print_error(f"L·ªói t·∫£i video: {e}")
            return False

    # ========== SCENE DETECTION ==========

    def _detect_scenes(self) -> List[Dict]:
        """Ph√°t hi·ªán scenes t·ª± ƒë·ªông"""
        if not self.video_path or not os.path.exists(self.video_path):
            return []

        print_progress("ƒêang ph√°t hi·ªán scenes...")

        try:
            cap = cv2.VideoCapture(self.video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            if total_frames == 0:
                print_error("Kh√¥ng th·ªÉ ƒë·ªçc video")
                return []

            prev_frame = None
            scene_boundaries = [0]
            frame_idx = 0

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # Downsample
                small_frame = cv2.resize(frame, (320, 180))
                gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY)

                if prev_frame is not None:
                    diff = cv2.absdiff(gray, prev_frame)
                    mean_diff = np.mean(diff)

                    if mean_diff > Config.SCENE_THRESHOLD:
                        if frame_idx - scene_boundaries[-1] > Config.MIN_SCENE_LENGTH:
                            scene_boundaries.append(frame_idx)

                prev_frame = gray
                frame_idx += 1

                # Progress
                if frame_idx % 100 == 0:
                    progress = (frame_idx / total_frames) * 100
                    print(f"\r  ƒêang x·ª≠ l√Ω: {progress:.1f}%", end='', flush=True)

            print()  # Newline
            scene_boundaries.append(total_frames - 1)
            cap.release()

            # Create scene info
            scenes = []
            for i in range(len(scene_boundaries) - 1):
                start = scene_boundaries[i]
                end = scene_boundaries[i + 1]
                scenes.append({
                    'scene_id': i,
                    'start_frame': start,
                    'end_frame': end,
                    'start_time': start / fps,
                    'end_time': end / fps,
                    'duration': (end - start) / fps
                })

            # B·ªé GI·ªöI H·∫†N - Ph√¢n t√≠ch t·∫•t c·∫£ scenes
            total_scenes = len(scenes)
            if total_scenes > Config.MAX_SCENES_TO_ANALYZE:
                print_warning(f"Video c√≥ {total_scenes} scenes - S·∫Ω ph√¢n t√≠ch T·∫§T C·∫¢ (kh√¥ng gi·ªõi h·∫°n)")
                # Kh√¥ng c·∫Øt n·ªØa!
            else:
                print_success(f"ƒê√£ ph√°t hi·ªán {total_scenes} scenes - S·∫Ω ph√¢n t√≠ch t·∫•t c·∫£")

            self.scenes = scenes
            return scenes

        except Exception as e:
            print_error(f"L·ªói ph√°t hi·ªán scenes: {e}")
            return []

    def _extract_frames_from_scenes(self) -> List[str]:
        """Tr√≠ch xu·∫•t key frames t·ª´ scenes"""
        if not self.scenes or not self.video_path:
            return []

        print_progress(f"ƒêang tr√≠ch xu·∫•t {Config.FRAMES_PER_SCENE} frames t·ª´ m·ªói scene...")

        try:
            cap = cv2.VideoCapture(self.video_path)
            all_frames = []

            for scene in self.scenes:
                scene_frames = []
                start = scene['start_frame']
                end = scene['end_frame']

                # L·∫•y frames ƒë·ªÅu
                positions = np.linspace(start, end, Config.FRAMES_PER_SCENE, dtype=int)

                for pos in positions:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, pos)
                    ret, frame = cap.read()

                    if ret:
                        frame_path = f"{Config.TEMP_FRAMES_DIR}/scene_{scene['scene_id']}_frame_{len(scene_frames)}.jpg"
                        cv2.imwrite(frame_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                        scene_frames.append(frame_path)

                scene['frames'] = scene_frames
                all_frames.extend(scene_frames)

            cap.release()
            self.frames = all_frames
            print_success(f"ƒê√£ tr√≠ch xu·∫•t {len(all_frames)} frames t·ª´ {len(self.scenes)} scenes")
            return all_frames

        except Exception as e:
            print_error(f"L·ªói tr√≠ch xu·∫•t frames: {e}")
            return []

    # ========== AUDIO ANALYSIS ==========

    def _extract_transcript(self) -> Optional[Dict]:
        """Tr√≠ch xu·∫•t transcript t·ª´ audio"""
        if not self.audio_path or not os.path.exists(self.audio_path):
            return None

        print_progress("ƒêang ph√¢n t√≠ch audio...")

        try:
            with open(self.audio_path, "rb") as audio_file:
                transcript = self.client.audio.transcriptions.create(
                    model=Config.WHISPER_MODEL,
                    file=audio_file,
                    response_format="verbose_json"
                )

            result = {
                'text': transcript.text,
                'language': getattr(transcript, 'language', 'unknown'),
                'duration': getattr(transcript, 'duration', 0),
            }

            print_success(f"Transcript: {len(result['text'])} k√Ω t·ª±")
            return result

        except Exception as e:
            print_warning(f"Kh√¥ng th·ªÉ tr√≠ch xu·∫•t transcript: {e}")
            return None

    # ========== VISUAL ANALYSIS ==========

    def _analyze_visual_composition(self, frame_path: str) -> Dict:
        """Ph√¢n t√≠ch m√†u s·∫Øc v√† composition"""
        try:
            img = cv2.imread(frame_path)
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            pixels = img_rgb.reshape(-1, 3)

            avg_color = np.mean(pixels, axis=0)
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            brightness = float(np.mean(gray))
            contrast = float(np.std(gray))

            # Classify mood
            r, g, b = avg_color
            if brightness < 85:
                mood = "dark, moody"
            elif brightness > 170:
                mood = "bright, airy"
            elif r > g and r > b:
                mood = "warm, energetic"
            elif b > r and b > g:
                mood = "cool, calm"
            else:
                mood = "balanced, natural"

            return {
                'brightness': brightness,
                'contrast': contrast,
                'color_mood': mood
            }
        except:
            return {}

    def _encode_image_base64(self, image_path: str) -> str:
        """Encode image to base64"""
        with open(image_path, "rb") as f:
            return base64.standard_b64encode(f.read()).decode("utf-8")

    # ========== AI ANALYSIS - C·∫¢I TI·∫æN ==========

    def _call_vision_api_with_retry(self, messages: List[Dict], max_tokens: int = 2000) -> Optional[str]:
        """G·ªçi Vision API v·ªõi retry logic"""
        for attempt in range(Config.MAX_RETRIES):
            try:
                response = self.client.chat.completions.create(
                    model=Config.VISION_MODEL,
                    max_tokens=max_tokens,
                    messages=messages
                )
                return response.choices[0].message.content
            except Exception as e:
                if attempt < Config.MAX_RETRIES - 1:
                    print_warning(f"API call failed (attempt {attempt + 1}/{Config.MAX_RETRIES}), retrying...")
                    time.sleep(Config.RETRY_DELAY * (attempt + 1))
                else:
                    print_error(f"API call failed after {Config.MAX_RETRIES} attempts: {e}")
                    return None
        return None

    def _analyze_scene(self, scene: Dict) -> Optional[Dict]:
        """Ph√¢n t√≠ch chi ti·∫øt m·ªôt scene - C·∫¢I TI·∫æN"""
        scene_id = scene['scene_id']
        frames = scene.get('frames', [])

        if not frames:
            return None

        print_progress(f"ƒêang ph√¢n t√≠ch scene {scene_id + 1}...", scene_id + 1, len(self.scenes))

        # PROMPT C·∫¢I TI·∫æN - CHI TI·∫æT NH√ÇN V·∫¨T V√Ä CON V·∫¨T
        content = [{
            "type": "text",
            "text": f"""Ph√¢n t√≠ch c·ª±c k·ª≥ chi ti·∫øt scene n√†y (th·ªùi l∆∞·ª£ng: {scene['duration']:.1f}s):

üé¨ H√ÄNH ƒê·ªòNG
- G√¨ ƒëang x·∫£y ra? Di·ªÖn bi·∫øn ch√≠nh?
- T·ªëc ƒë·ªô h√†nh ƒë·ªông (ch·∫≠m/v·ª´a/nhanh)?

üë§ NH√ÇN V·∫¨T (N·∫æU C√ì) - M√î T·∫¢ C·ª∞C K·ª≤ CHI TI·∫æT:
- S·ªë l∆∞·ª£ng: Bao nhi√™u ng∆∞·ªùi?
- Gi·ªõi t√≠nh v√† tu·ªïi ∆∞·ªõc t√≠nh (VD: nam 25-30 tu·ªïi, n·ªØ 40s)
- Chi·ªÅu cao ∆∞·ªõc t√≠nh (VD: 170cm, tall ~185cm, short ~155cm)
- Th·ªÉ h√¨nh: (slim/athletic/muscular/average/heavyset/petite) v√† c√¢n n·∫∑ng ∆∞·ªõc t√≠nh
- M√†u da: (pale/fair/tan/olive/brown/dark brown/black) - m√¥ t·∫£ ch√≠nh x√°c
- T√≥c:
  * M√†u: (blonde/brown/black/red/gray/white/dyed)
  * Ki·ªÉu: (straight/wavy/curly/braided/ponytail/bun/short/long)
  * ƒê·ªô d√†i: (buzz cut/short/shoulder-length/long/very long)
- ƒê·∫∑c ƒëi·ªÉm khu√¥n m·∫∑t: (angular/round/oval/square), m·∫Øt, m≈©i, mi·ªáng
- T·ª∑ l·ªá c∆° th·ªÉ: (proportions) - ƒë·∫ßu:c∆° th·ªÉ, ch√¢n d√†i/ng·∫Øn
- Trang ph·ª•c CHI TI·∫æT:
  * Lo·∫°i: (casual/formal/sportswear/uniform/traditional)
  * M√†u s·∫Øc ch·ªß ƒë·∫°o v√† h·ªça ti·∫øt
  * Form d√°ng: (fitted/loose/oversized/tight)
  * Ph·ª• ki·ªán: m≈©/k√≠nh/ƒë·ªìng h·ªì/trang s·ª©c
- T∆∞ th·∫ø v√† ng√¥n ng·ªØ c∆° th·ªÉ
- C·∫£m x√∫c tr√™n khu√¥n m·∫∑t

üêæ CON V·∫¨T (N·∫æU C√ì) - M√î T·∫¢ C·ª∞C K·ª≤ CHI TI·∫æT:
- Lo√†i: (ch√≥/m√®o/chim/etc.) v√† gi·ªëng c·ª• th·ªÉ n·∫øu nh·∫≠n ra
- K√≠ch th∆∞·ªõc: (tiny/small/medium/large/giant) + ∆∞·ªõc t√≠nh chi·ªÅu d√†i/cao
- C√¢n n·∫∑ng ∆∞·ªõc t√≠nh: (VD: 5kg cat, 25kg dog, 500g bird)
- M√†u s·∫Øc l√¥ng/v·∫£y/l√¥ng v≈©: m√¥ t·∫£ chi ti·∫øt c√°c m√†u v√† h·ªça ti·∫øt
- ƒê·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t: tai/ƒëu√¥i/m·∫Øt/m·ªè/m√≥ng vu·ªët
- T·ª∑ l·ªá c∆° th·ªÉ: ƒë·∫ßu:th√¢n, ch√¢n d√†i/ng·∫Øn, ƒëu√¥i d√†i/ng·∫Øn
- T∆∞ th·∫ø v√† h√†nh ƒë·ªông ƒëang l√†m

üèûÔ∏è B·ªêI C·∫¢NH
- ƒê·ªãa ƒëi·ªÉm: trong nh√†/ngo√†i tr·ªùi, m√¥i tr∆∞·ªùng g√¨?
- Kh√¥ng gian: r·ªông/h·∫πp, ki·∫øn tr√∫c/thi√™n nhi√™n
- V·∫≠t th·ªÉ xung quanh quan tr·ªçng

üì∑ CAMERA & K·ª∏ THU·∫¨T
- G√≥c quay: eye-level/high-angle/low-angle/bird's-eye/worm's-eye
- Di chuy·ªÉn: static/pan/tilt/zoom/dolly/tracking/handheld/crane
- Shot type: wide/full/medium/close-up/extreme close-up
- ƒê·ªô s√¢u tr∆∞·ªùng ·∫£nh: shallow/deep DOF

üí° √ÅNH S√ÅNG & M√ÄU S·∫ÆC
- Ngu·ªìn s√°ng: natural/artificial/mixed, h∆∞·ªõng √°nh s√°ng
- Ch·∫•t l∆∞·ª£ng: soft/hard/dramatic/flat
- Color grading: warm/cool/neutral/vibrant/desaturated
- B·∫ßu kh√¥ng kh√≠: bright/moody/mysterious/romantic

üé® COMPOSITION
- Quy t·∫Øc: rule of thirds/golden ratio/symmetry/leading lines
- C√¢n b·∫±ng: balanced/asymmetrical
- Layers: foreground/middle/background elements
- Depth: perspective v√† ƒë·ªô s√¢u kh√¥ng gian

üòä C·∫¢M X√öC & MOOD
- T√¢m tr·∫°ng t·ªïng th·ªÉ c·ªßa scene
- C·∫£m gi√°c m√† scene g·ª£i l√™n

Tr·∫£ l·ªùi TI·∫æNG VI·ªÜT, chi ti·∫øt, ch√≠nh x√°c, d·ª±a tr√™n nh·ªØng g√¨ nh√¨n th·∫•y."""
        }]

        # Add frames
        for frame_path in frames:
            try:
                base64_img = self._encode_image_base64(frame_path)
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_img}",
                        "detail": "high"
                    }
                })
            except:
                continue

        # Call API v·ªõi max_tokens cao h∆°n
        analysis = self._call_vision_api_with_retry([{"role": "user", "content": content}], max_tokens=2500)

        if not analysis:
            return None

        # Visual composition
        visual = self._analyze_visual_composition(frames[0])

        return {
            'scene_id': scene_id,
            'analysis': analysis,
            'visual_composition': visual,
            'duration': scene['duration'],
            'timestamp': f"{scene['start_time']:.1f}s - {scene['end_time']:.1f}s"
        }

    def _analyze_overall(self, scene_analyses: List[Dict], transcript: Optional[Dict]) -> Optional[str]:
        """Ph√¢n t√≠ch t·ªïng th·ªÉ video - C·∫¢I TI·∫æN"""
        print_progress("ƒêang t·ªïng h·ª£p ph√¢n t√≠ch t·ªïng th·ªÉ...")

        # Prepare scene summaries
        scene_text = "\n\n".join([
            f"SCENE {s['scene_id'] + 1} ({s['timestamp']}):\n{s['analysis']}"
            for s in scene_analyses if s
        ])

        # Transcript
        transcript_text = ""
        if transcript:
            transcript_text = f"\n\nTRANSCRIPT:\n{transcript.get('text', '')}"

        # Metadata
        metadata_text = ""
        if self.video_metadata:
            metadata_text = f"\nTh·ªùi l∆∞·ª£ng: {self.video_metadata.get('duration', 0)}s"

        prompt = f"""D·ª±a tr√™n ph√¢n t√≠ch chi ti·∫øt t·ª´ng scene v√† transcript, vi·∫øt t·ªïng h·ª£p ph√¢n t√≠ch video:

VIDEO METADATA:{metadata_text}

PH√ÇN T√çCH T·ª™NG SCENE:
{scene_text}
{transcript_text}

H√£y vi·∫øt ph√¢n t√≠ch t·ªïng th·ªÉ bao g·ªìm:

1. T√ìM T·∫ÆT N·ªòI DUNG
- C·ªët truy·ªán ch√≠nh, di·ªÖn bi·∫øn
- Th√¥ng ƒëi·ªáp/ch·ªß ƒë·ªÅ

2. NH√ÇN V·∫¨T CHI TI·∫æT (n·∫øu c√≥)
- T·ªïng h·ª£p t·∫•t c·∫£ nh√¢n v·∫≠t xu·∫•t hi·ªán
- M√¥ t·∫£ ngo·∫°i h√¨nh, trang ph·ª•c, ƒë·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng ng∆∞·ªùi
- T·ª∑ l·ªá c∆° th·ªÉ, m√†u s·∫Øc, phong c√°ch
- Vai tr√≤ v√† h√†nh ƒë·ªông

3. CON V·∫¨T CHI TI·∫æT (n·∫øu c√≥)
- T·ªïng h·ª£p t·∫•t c·∫£ con v·∫≠t
- Lo√†i, k√≠ch th∆∞·ªõc, m√†u s·∫Øc, ƒë·∫∑c ƒëi·ªÉm
- T·ª∑ l·ªá c∆° th·ªÉ, h√†nh vi

4. PHONG C√ÅCH H√åNH ·∫¢NH
- Visual style t·ªïng th·ªÉ
- M√†u s·∫Øc ch·ªß ƒë·∫°o
- Lighting approach
- Composition patterns

5. K·ª∏ THU·∫¨T QUAY
- Camera movements ch√≠nh
- Shot types s·ª≠ d·ª•ng
- Transitions gi·ªØa scenes
- Tempo v√† rhythm

6. KH√îNG KH√ç & C·∫¢M X√öC
- Mood t·ªïng th·ªÉ
- Tone (dramatic/comedic/serious/etc.)
- Emotional arc

7. TH·ªÇ LO·∫†I & PHONG C√ÅCH
- Documentary/Narrative/Music Video/Commercial/Tutorial/etc.
- Reference style (cinematic/documentary/social media/etc.)

8. ƒê·∫∂C ƒêI·ªÇM N·ªîI B·∫¨T
- ƒêi·ªÉm ƒë·∫∑c bi·ªát, unique elements
- Techniques ƒë√°ng ch√∫ √Ω
- Visual motifs

Tr·∫£ l·ªùi chi ti·∫øt, c√≥ c·∫•u tr√∫c, TI·∫æNG VI·ªÜT."""

        return self._call_vision_api_with_retry(
            [{"role": "user", "content": prompt}],
            max_tokens=3000
        )

    def _generate_prompts(self, overall_analysis: str, scene_analyses: List[Dict]) -> Optional[str]:
        """T·∫°o Sora prompts - C·∫¢I TI·∫æN"""
        print_progress("ƒêang t·∫°o Sora 2 prompts v·ªõi m√¥ t·∫£ chi ti·∫øt...")

        # Scene summaries - l·∫•y nhi·ªÅu scenes h∆°n
        scene_text = "\n".join([
            f"Scene {s['scene_id'] + 1}: {s['analysis'][:400]}..."
            for s in scene_analyses[:10] if s  # TƒÉng t·ª´ 5 l√™n 10 scenes
        ])

        prompt = f"""D·ª±a tr√™n ph√¢n t√≠ch chi ti·∫øt video, t·∫°o 3 PROMPT cho Sora 2:

PH√ÇN T√çCH T·ªîNG TH·ªÇ:
{overall_analysis}

C√ÅC SCENE CHI TI·∫æT:
{scene_text}

T·∫°o 3 prompts:

1. **SHORT PROMPT** (60-80 words): S√∫c t√≠ch nh∆∞ng c√≥ detail quan tr·ªçng

2. **DETAILED PROMPT** (150-200 words):
   - M√¥ t·∫£ CH√çNH X√ÅC nh√¢n v·∫≠t (chi·ªÅu cao, body type, m√†u da, t√≥c, qu·∫ßn √°o, t·ª∑ l·ªá)
   - M√¥ t·∫£ CH√çNH X√ÅC con v·∫≠t (lo√†i, size, m√†u s·∫Øc, proportions)
   - Camera movement v√† angles c·ª• th·ªÉ
   - Lighting setup chi ti·∫øt
   - Environment v√† atmosphere
   - Action v√† movement

3. **CINEMATIC PROMPT** (120-160 words):
   - Ngh·ªá thu·∫≠t, metaphor
   - Film references n·∫øu ph√π h·ª£p
   - Emotional tone
   - Artistic techniques
   - Chi ti·∫øt visual composition

Y√äU C·∫¶U QUAN TR·ªåNG:
‚úÖ T·∫§T C·∫¢ B·∫∞NG TI·∫æNG ANH
‚úÖ M√¥ t·∫£ nh√¢n v·∫≠t/con v·∫≠t PH·∫¢I c·ª±c k·ª≥ chi ti·∫øt: height, build, skin tone, hair (color/style/length), clothing (color/style/fit), proportions
‚úÖ Camera: c·ª• th·ªÉ movement type (dolly/crane/steadicam/handheld)
‚úÖ Lighting: c·ª• th·ªÉ (soft key light, rim light, practical lights, etc.)
‚úÖ R√µ r√†ng, sinh ƒë·ªông, c√≥ th·ªÉ visualize ƒë∆∞·ª£c
‚úÖ Kh√¥ng gi·∫£i th√≠ch, CH·ªà vi·∫øt prompts

Format:
=== SHORT PROMPT ===
[prompt here]

=== DETAILED PROMPT ===
[prompt here]

=== CINEMATIC PROMPT ===
[prompt here]"""

        return self._call_vision_api_with_retry(
            [{"role": "user", "content": prompt}],
            max_tokens=2000
        )

    # ========== EXPORT ==========

    def _save_results(self, overall: str, scenes: List[Dict], transcript: Optional[Dict], prompts: str) -> str:
        """L∆∞u k·∫øt qu·∫£ ra files"""
        try:
            # Safe filename
            safe_title = "".join(c for c in self.video_title if c.isalnum() or c in (' ', '_', '-'))[:50]
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{safe_title}_{timestamp}"

            # ========== TXT FILE ==========
            txt_path = f"{Config.OUTPUT_DIR}/{filename}.txt"

            scene_details = "\n\n".join([
                f"""{'='*60}
SCENE {s['scene_id'] + 1} | {s['timestamp']} | {s['duration']:.1f}s
{'='*60}

{s['analysis']}

VISUAL METRICS:
- Brightness: {s.get('visual_composition', {}).get('brightness', 0):.1f}
- Contrast: {s.get('visual_composition', {}).get('contrast', 0):.1f}
- Color Mood: {s.get('visual_composition', {}).get('color_mood', 'N/A')}
"""
                for s in scenes if s
            ])

            transcript_section = ""
            if transcript:
                transcript_section = f"""
{'='*60}
TRANSCRIPT
{'='*60}
Language: {transcript.get('language', 'unknown')}

{transcript.get('text', '')}
"""

            txt_content = f"""{'='*70}
YOUTUBE TO SORA 2 - DETAILED ANALYSIS REPORT v2.1
{'='*70}

VIDEO: {self.video_title}
URL: {self.youtube_url}
DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Duration: {self.video_metadata.get('duration', 0)}s
Total Scenes Analyzed: {len(scenes)}

{'='*70}
OVERALL ANALYSIS
{'='*70}

{overall}

{'='*70}
SCENE-BY-SCENE ANALYSIS ({len(scenes)} scenes)
{'='*70}

{scene_details}
{transcript_section}

{'='*70}
SORA 2 PROMPTS (DETAILED CHARACTER & ANIMAL DESCRIPTIONS)
{'='*70}

{prompts}

{'='*70}
Generated by YouTube to Sora 2 Analyzer v2.1
Features: Unlimited scenes, detailed character/animal analysis
{'='*70}
"""

            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(txt_content)

            print_success(f"ƒê√£ l∆∞u TXT: {txt_path}")

            # ========== JSON FILE ==========
            json_path = f"{Config.OUTPUT_DIR}/{filename}.json"
            json_data = {
                'video_info': {
                    'title': self.video_title,
                    'url': self.youtube_url,
                    'metadata': self.video_metadata,
                    'date': datetime.now().isoformat(),
                    'total_scenes': len(scenes)
                },
                'overall_analysis': overall,
                'scenes': scenes,
                'transcript': transcript,
                'sora_prompts': prompts,
                'version': '2.1',
                'features': ['unlimited_scenes', 'detailed_character_analysis', 'detailed_animal_analysis']
            }

            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)

            print_success(f"ƒê√£ l∆∞u JSON: {json_path}")

            # ========== MARKDOWN FILE ==========
            md_path = f"{Config.OUTPUT_DIR}/{filename}.md"
            md_content = f"""# YouTube to Sora 2 - Analysis Report

**Version:** 2.1 (Detailed Character & Animal Analysis)
**Video:** {self.video_title}
**URL:** {self.youtube_url}
**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Duration:** {self.video_metadata.get('duration', 0)}s
**Total Scenes:** {len(scenes)}

---

## üìä Overall Analysis

{overall}

---

## üé¨ Scene-by-Scene Breakdown

"""
            for s in scenes:
                if s:
                    md_content += f"""### Scene {s['scene_id'] + 1} ({s['timestamp']}, {s['duration']:.1f}s)

{s['analysis']}

**Visual Metrics:**
- Brightness: {s.get('visual_composition', {}).get('brightness', 0):.1f}
- Contrast: {s.get('visual_composition', {}).get('contrast', 0):.1f}
- Color Mood: {s.get('visual_composition', {}).get('color_mood', 'N/A')}

---

"""

            if transcript:
                md_content += f"""## üé§ Transcript

**Language:** {transcript.get('language', 'unknown')}

{transcript.get('text', '')}

---

"""

            md_content += f"""## üé® Sora 2 Prompts

{prompts}

---

*Generated by YouTube to Sora 2 Analyzer v2.1*
*Features: Unlimited scenes analysis, detailed character/animal descriptions, body proportions, color details*
"""

            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(md_content)

            print_success(f"ƒê√£ l∆∞u Markdown: {md_path}")

            return txt_path

        except Exception as e:
            print_error(f"L·ªói l∆∞u file: {e}")
            return ""

    # ========== MAIN PROCESS ==========

    def analyze(self, youtube_url: str, use_cache: bool = True, analyze_audio: bool = True) -> Optional[Dict]:
        """
        Ph√¢n t√≠ch video YouTube v√† t·∫°o Sora prompts

        Args:
            youtube_url: URL c·ªßa video YouTube
            use_cache: S·ª≠ d·ª•ng cache n·∫øu c√≥
            analyze_audio: Ph√¢n t√≠ch audio/transcript

        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ph√¢n t√≠ch ho·∫∑c None n·∫øu th·∫•t b·∫°i
        """
        self.youtube_url = youtube_url

        print_header("YOUTUBE TO SORA 2 - ADVANCED ANALYZER v2.1")
        print("‚ú® Features: Unlimited scenes, detailed character & animal analysis")

        # Check cache
        cache_key = self._get_cache_key(youtube_url)
        if use_cache:
            cached = self._load_cache(cache_key)
            if cached:
                print_success("T√¨m th·∫•y k·∫øt qu·∫£ trong cache!")
                self.video_title = cached.get('video_info', {}).get('title', '')
                print(f"Video: {self.video_title}\n")

                print_section("PH√ÇN T√çCH T·ªîNG TH·ªÇ (t·ª´ cache)")
                print(cached.get('overall_analysis', ''))

                print_section("SORA 2 PROMPTS (t·ª´ cache)")
                print(cached.get('sora_prompts', ''))

                return cached

        # Step 1: Get metadata
        self._get_video_metadata(youtube_url)

        # Step 2: Download
        if not self._download_video(youtube_url):
            return None

        # Step 3: Scene detection
        if not self._detect_scenes():
            cleanup_temp_files()
            return None

        # Step 4: Extract frames
        if not self._extract_frames_from_scenes():
            cleanup_temp_files()
            return None

        # Step 5: Transcript (optional)
        transcript = None
        if analyze_audio:
            transcript = self._extract_transcript()

        # Step 6: Analyze scenes
        print_section(f"PH√ÇN T√çCH CHI TI·∫æT {len(self.scenes)} SCENES")
        scene_analyses = []
        for scene in self.scenes:
            result = self._analyze_scene(scene)
            if result:
                scene_analyses.append(result)
            # Small delay ƒë·ªÉ tr√°nh rate limit
            time.sleep(0.5)

        if not scene_analyses:
            print_error("Kh√¥ng th·ªÉ ph√¢n t√≠ch scenes")
            cleanup_temp_files()
            return None

        print_success(f"ƒê√£ ph√¢n t√≠ch xong {len(scene_analyses)} scenes!")

        # Step 7: Overall analysis
        print_section("PH√ÇN T√çCH T·ªîNG TH·ªÇ")
        overall = self._analyze_overall(scene_analyses, transcript)

        if overall:
            print(overall)
        else:
            cleanup_temp_files()
            return None

        # Step 8: Generate prompts
        print_section("T·∫†O SORA 2 PROMPTS")
        prompts = self._generate_prompts(overall, scene_analyses)

        if prompts:
            print(prompts)
        else:
            cleanup_temp_files()
            return None

        # Step 9: Save results
        print_section("L∆ØU K·∫æT QU·∫¢")
        self._save_results(overall, scene_analyses, transcript, prompts)

        # Save cache
        if use_cache:
            cache_data = {
                'video_info': {
                    'title': self.video_title,
                    'url': self.youtube_url,
                    'metadata': self.video_metadata,
                    'total_scenes': len(scene_analyses)
                },
                'overall_analysis': overall,
                'scene_analyses': scene_analyses,
                'transcript': transcript,
                'sora_prompts': prompts,
                'version': '2.1'
            }
            self._save_cache(cache_key, cache_data)
            print_success("ƒê√£ l∆∞u v√†o cache")

        # Cleanup
        cleanup_temp_files()

        print_header("‚úì HO√ÄN T·∫§T!")
        print(f"üìÅ K·∫øt qu·∫£ ƒë√£ l∆∞u trong folder: {Config.OUTPUT_DIR}/")
        print(f"üìä ƒê√£ ph√¢n t√≠ch {len(scene_analyses)} scenes v·ªõi m√¥ t·∫£ chi ti·∫øt nh√¢n v·∫≠t & con v·∫≠t\n")

        return {
            'overall_analysis': overall,
            'scene_analyses': scene_analyses,
            'transcript': transcript,
            'sora_prompts': prompts
        }


# ========== CLI ==========

def main():
    """Main CLI interface"""
    print_header("YOUTUBE TO SORA 2 - ADVANCED ANALYZER v2.1")
    print("‚ú® Kh√¥ng gi·ªõi h·∫°n scenes")
    print("‚ú® Ph√¢n t√≠ch chi ti·∫øt nh√¢n v·∫≠t: chi·ªÅu cao, c√¢n n·∫∑ng, m√†u da, t√≥c, trang ph·ª•c")
    print("‚ú® Ph√¢n t√≠ch chi ti·∫øt con v·∫≠t: lo√†i, k√≠ch th∆∞·ªõc, m√†u s·∫Øc, t·ª∑ l·ªá c∆° th·ªÉ\n")

    # Input URL
    youtube_url = input("Nh·∫≠p YouTube URL: ").strip()
    if not youtube_url:
        print_error("URL kh√¥ng h·ª£p l·ªá")
        return

    # Check API key
    load_env_file()
    api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        api_key = input("Nh·∫≠p OpenAI API Key: ").strip()
        if not api_key:
            print_error("C·∫ßn OpenAI API Key")
            print("\nC√°ch l·∫•y API key:")
            print("1. V√†o https://platform.openai.com/api-keys")
            print("2. T·∫°o key m·ªõi")
            print("3. L∆∞u v√†o file .env ho·∫∑c nh·∫≠p tr·ª±c ti·∫øp")
            return

    # Options
    print("\n--- T√ôY CH·ªåN ---")
    use_cache = input("S·ª≠ d·ª•ng cache? (y/n, m·∫∑c ƒë·ªãnh: y): ").strip().lower() != 'n'
    analyze_audio = input("Ph√¢n t√≠ch audio? (y/n, m·∫∑c ƒë·ªãnh: y): ").strip().lower() != 'n'

    # Process
    try:
        analyzer = YouTubeToSoraAnalyzer(api_key=api_key)
        result = analyzer.analyze(
            youtube_url=youtube_url,
            use_cache=use_cache,
            analyze_audio=analyze_audio
        )

        if result:
            print_success("Ph√¢n t√≠ch th√†nh c√¥ng!")
            print(f"\nüìä Th·ªëng k√™:")
            print(f"  - T·ªïng scenes: {len(result['scene_analyses'])}")
            print(f"  - Transcript: {'C√≥' if result['transcript'] else 'Kh√¥ng'}")
        else:
            print_error("C√≥ l·ªói x·∫£y ra")

    except KeyboardInterrupt:
        print("\n\n‚ö† ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        cleanup_temp_files()
    except Exception as e:
        print_error(f"L·ªói: {e}")
        import traceback
        traceback.print_exc()
        cleanup_temp_files()


if __name__ == "__main__":
    main()
