#!/usr/bin/env python3
"""
YouTube Scene-by-Scene Analyzer - Per-Scene Prompt Generator
Ph√¢n t√≠ch t·ª´ng c·∫£nh ri√™ng bi·ªát v√† t·∫°o prompt chi ti·∫øt cho m·ªói c·∫£nh

Features:
- Xu·∫•t ·∫£nh ƒë·∫ßu + ·∫£nh cu·ªëi cho m·ªói c·∫£nh
- T·∫°o prompt cinema chi ti·∫øt ri√™ng cho M·ªñI c·∫£nh
- Export organized by scenes
"""

import os
import sys
import time
import subprocess
import cv2
import base64
import numpy as np
from pathlib import Path
import json
from openai import OpenAI
from datetime import datetime
import hashlib
from typing import List, Dict, Optional
import shutil

# ========== CONFIGURATION ==========

class Config:
    """C·∫•u h√¨nh"""
    SCENE_THRESHOLD = 30.0
    MIN_SCENE_LENGTH = 15
    MAX_VIDEO_HEIGHT = 1080

    VISION_MODEL = "gpt-4o"
    WHISPER_MODEL = "whisper-1"
    MAX_RETRIES = 3
    RETRY_DELAY = 2

    CACHE_DIR = "cache"
    OUTPUT_DIR = "output_scenes"
    TEMP_FRAMES_DIR = "temp_frames"

    MAX_SCENES_TO_ANALYZE = 999

# ========== UTILITIES ==========

def load_env_file():
    """Load .env file"""
    env_file = Path('.env')
    if env_file.exists():
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    key, value = key.strip(), value.strip()
                    if value.startswith('"') and value.endswith('"'):
                        value = value[1:-1]
                    if value.startswith("'") and value.endswith("'"):
                        value = value[1:-1]
                    os.environ[key] = value

def ensure_directories():
    """T·∫°o th∆∞ m·ª•c"""
    for d in [Config.CACHE_DIR, Config.OUTPUT_DIR, Config.TEMP_FRAMES_DIR]:
        Path(d).mkdir(exist_ok=True)

def cleanup_temp():
    """X√≥a temp files"""
    for f in ["temp_video.mp4", "temp_audio.m4a"]:
        if os.path.exists(f):
            try:
                os.remove(f)
            except:
                pass
    if os.path.exists(Config.TEMP_FRAMES_DIR):
        try:
            shutil.rmtree(Config.TEMP_FRAMES_DIR)
            Path(Config.TEMP_FRAMES_DIR).mkdir(exist_ok=True)
        except:
            pass

def print_header(text: str):
    print("\n" + "="*70)
    print(text.center(70))
    print("="*70 + "\n")

def print_progress(msg: str, step: int = None, total: int = None):
    if step and total:
        print(f"[{step}/{total}] {msg}")
    else:
        print(f"‚Ä¢ {msg}")

def print_success(msg: str):
    print(f"‚úì {msg}")

def print_error(msg: str):
    print(f"‚úó {msg}")

# ========== MAIN ANALYZER ==========

class SceneBySceneAnalyzer:
    """Ph√¢n t√≠ch t·ª´ng c·∫£nh ri√™ng bi·ªát"""

    SCENE_PROMPT_TEMPLATE = """Ph√¢n t√≠ch scene n√†y ({duration:.1f}s) v√† t·∫°o M·ªòT PROMPT DUY NH·∫§T cho Sora 2 theo TI√äU CHU·∫®N HOLLYWOOD CINEMA.

B·∫°n ƒëang xem 2 frames: FRAME ƒê·∫¶U v√† FRAME CU·ªêI c·ªßa scene n√†y.

T·∫°o prompt TI·∫æNG ANH, chi ti·∫øt, 150-200 words, bao g·ªìm:

üìπ CAMERA & COMPOSITION:
- Shot type & size (wide/medium/close-up/etc)
- Camera movement (static/pan/tilt/dolly/tracking/crane/steadicam/handheld)
- Camera angle (eye-level/high/low/dutch/overhead)
- Lens focal length estimate (18mm/35mm/50mm/85mm/100mm+)
- Aperture & DOF (shallow f/1.4-2.8 / deep f/8-16)
- Composition (rule of thirds/centered/symmetrical)
- Aspect ratio (16:9/2.39:1/1.85:1)

üë• CHARACTERS (n·∫øu c√≥):
- Physical: gender, age estimate, height (~cm), build (slim/athletic/muscular/heavy), weight estimate
- Skin tone: ivory/fair/tan/olive/bronze/brown/deep brown/ebony
- Hair: color (blonde/brown/black/red/gray), style (straight/wavy/curly/braided), length (short/medium/long)
- Facial features: angular/soft/chiseled/round, jawline, eyes, nose
- Costume: style, colors, materials, fit, accessories
- Action/performance: what they're doing, body language, expression

üêæ ANIMALS/CREATURES (n·∫øu c√≥):
- Species, breed, size (height/length/weight), coat color & pattern, features, movement

üí° LIGHTING:
- Setup type (3-point/natural/high-key/low-key/Rembrandt/butterfly)
- Key light position & quality (hard/soft)
- Fill ratio (high contrast/balanced)
- Color temperature (warm 3200K/neutral/cool 5600K)
- Practicals (visible light sources)
- Atmosphere (haze/fog/volumetric)

üé® COLOR GRADING:
- Palette (warm/cool/complementary/monochromatic)
- LUT style (naturalistic/teal-orange/bleach bypass/noir)
- Saturation level (vibrant/muted/desaturated)
- Contrast & blacks (crushed/lifted/normal)

üèûÔ∏è ENVIRONMENT & PRODUCTION:
- Location type (interior/exterior/urban/nature)
- Set design & props
- Time of day & weather
- VFX elements (CGI/practical/none)

üé≠ ACTION & STORY:
- What's happening in the scene
- Movement & pacing (slow/medium/fast)
- Emotional tone & mood
- Genre indicators

üé¨ STYLE REFERENCE:
- Comparable to which film/director style
- Production value level (indie/studio/blockbuster)

CRITICAL REQUIREMENTS:
‚úÖ Write in ENGLISH only
‚úÖ Use professional cinema terminology
‚úÖ Be SPECIFIC with numbers (heights in cm, focal lengths in mm, color temps in K)
‚úÖ Describe visual evidence you see, not assumptions
‚úÖ Create ONE continuous paragraph prompt (not bullet points)
‚úÖ Focus on recreatable technical details for Sora 2
‚úÖ 150-200 words total

Format your response as:
PROMPT: [your single-paragraph detailed prompt here]"""

    def __init__(self, api_key: Optional[str] = None):
        load_env_file()

        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI API key not found!")

        self.client = OpenAI(api_key=api_key)
        ensure_directories()

        self.video_path: Optional[str] = None
        self.youtube_url: Optional[str] = None
        self.video_title: str = ""
        self.video_metadata: Dict = {}
        self.scenes: List[Dict] = []

    # ========== VIDEO DOWNLOAD ==========

    def _get_metadata(self, url: str) -> Dict:
        print_progress("ƒêang l·∫•y metadata...")
        try:
            result = subprocess.run(
                ["yt-dlp", "--dump-json", url],
                capture_output=True,
                text=True,
                timeout=30
            )
            if result.returncode == 0:
                metadata = json.loads(result.stdout)
                self.video_title = metadata.get('title', 'Unknown')
                self.video_metadata = {
                    'title': metadata.get('title'),
                    'duration': metadata.get('duration', 0),
                    'width': metadata.get('width'),
                    'height': metadata.get('height'),
                    'fps': metadata.get('fps'),
                }
                print_success(f"Video: {self.video_title}")
                return self.video_metadata
        except Exception as e:
            print_error(f"L·ªói metadata: {e}")

        self.video_title = f"Video_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return {}

    def _download_video(self, url: str) -> bool:
        print_progress("ƒêang t·∫£i video...")
        try:
            result = subprocess.run(
                [
                    "yt-dlp",
                    "-f", f"best[height<={Config.MAX_VIDEO_HEIGHT}][ext=mp4]",
                    "-o", "temp_video.mp4",
                    url
                ],
                capture_output=True,
                text=True,
                timeout=300
            )
            if result.returncode != 0:
                print_error(f"L·ªói t·∫£i video: {result.stderr}")
                return False

            self.video_path = "temp_video.mp4"
            print_success("Video ƒë√£ t·∫£i xong")
            return True
        except Exception as e:
            print_error(f"L·ªói: {e}")
            return False

    # ========== SCENE DETECTION ==========

    def _detect_scenes(self) -> List[Dict]:
        if not self.video_path or not os.path.exists(self.video_path):
            return []

        print_progress("ƒêang ph√°t hi·ªán scenes...")

        try:
            cap = cv2.VideoCapture(self.video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            if total_frames == 0:
                print_error("Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c video")
                return []

            prev_frame = None
            scene_boundaries = [0]
            frame_idx = 0

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                small = cv2.resize(frame, (320, 180))
                gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)

                if prev_frame is not None:
                    diff = cv2.absdiff(gray, prev_frame)
                    mean_diff = np.mean(diff)

                    if mean_diff > Config.SCENE_THRESHOLD:
                        if frame_idx - scene_boundaries[-1] > Config.MIN_SCENE_LENGTH:
                            scene_boundaries.append(frame_idx)

                prev_frame = gray
                frame_idx += 1

                if frame_idx % 100 == 0:
                    progress = (frame_idx / total_frames) * 100
                    print(f"\r  ƒêang x·ª≠ l√Ω: {progress:.1f}%", end='', flush=True)

            print()
            scene_boundaries.append(total_frames - 1)
            cap.release()

            scenes = []
            for i in range(len(scene_boundaries) - 1):
                start = scene_boundaries[i]
                end = scene_boundaries[i + 1]
                scenes.append({
                    'scene_id': i,
                    'start_frame': start,
                    'end_frame': end,
                    'start_time': start / fps,
                    'end_time': end / fps,
                    'duration': (end - start) / fps
                })

            print_success(f"ƒê√£ ph√°t hi·ªán {len(scenes)} scenes")
            self.scenes = scenes
            return scenes

        except Exception as e:
            print_error(f"L·ªói ph√°t hi·ªán scenes: {e}")
            return []

    # ========== EXTRACT FIRST & LAST FRAMES ==========

    def _extract_first_last_frames(self) -> bool:
        """Tr√≠ch xu·∫•t frame ƒë·∫ßu v√† frame cu·ªëi c·ªßa m·ªói scene"""
        if not self.scenes or not self.video_path:
            return False

        print_progress(f"ƒêang tr√≠ch xu·∫•t frame ƒê·∫¶U + CU·ªêI t·ª´ {len(self.scenes)} scenes...")

        try:
            cap = cv2.VideoCapture(self.video_path)

            for scene in self.scenes:
                scene_id = scene['scene_id']
                start_frame = scene['start_frame']
                end_frame = scene['end_frame']

                # Frame ƒê·∫¶U
                cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
                ret, frame = cap.read()
                if ret:
                    first_path = f"{Config.TEMP_FRAMES_DIR}/scene_{scene_id:04d}_FIRST.jpg"
                    cv2.imwrite(first_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                    scene['frame_first'] = first_path

                # Frame CU·ªêI
                cap.set(cv2.CAP_PROP_POS_FRAMES, end_frame)
                ret, frame = cap.read()
                if ret:
                    last_path = f"{Config.TEMP_FRAMES_DIR}/scene_{scene_id:04d}_LAST.jpg"
                    cv2.imwrite(last_path, frame, [cv2.IMWRITE_JPEG_QUALITY, 95])
                    scene['frame_last'] = last_path

            cap.release()
            print_success(f"ƒê√£ tr√≠ch xu·∫•t {len(self.scenes) * 2} frames (ƒë·∫ßu + cu·ªëi)")
            return True

        except Exception as e:
            print_error(f"L·ªói tr√≠ch xu·∫•t frames: {e}")
            return False

    # ========== AI ANALYSIS ==========

    def _encode_image_base64(self, image_path: str) -> str:
        with open(image_path, "rb") as f:
            return base64.standard_b64encode(f.read()).decode("utf-8")

    def _call_vision_api(self, messages: List[Dict], max_tokens: int = 500) -> Optional[str]:
        for attempt in range(Config.MAX_RETRIES):
            try:
                response = self.client.chat.completions.create(
                    model=Config.VISION_MODEL,
                    max_tokens=max_tokens,
                    messages=messages
                )
                return response.choices[0].message.content
            except Exception as e:
                if attempt < Config.MAX_RETRIES - 1:
                    print(f"  Retry {attempt + 1}...", end='', flush=True)
                    time.sleep(Config.RETRY_DELAY * (attempt + 1))
                else:
                    print_error(f"API failed: {e}")
                    return None
        return None

    def _analyze_scene_and_generate_prompt(self, scene: Dict) -> Optional[Dict]:
        """Ph√¢n t√≠ch scene v√† t·∫°o prompt ri√™ng cho scene ƒë√≥"""
        scene_id = scene['scene_id']
        frame_first = scene.get('frame_first')
        frame_last = scene.get('frame_last')

        if not frame_first or not frame_last:
            return None

        print_progress(f"Analyzing scene {scene_id + 1}...", scene_id + 1, len(self.scenes))

        # Build content
        content = [{
            "type": "text",
            "text": self.SCENE_PROMPT_TEMPLATE.format(duration=scene['duration'])
        }]

        # Add FIRST frame
        try:
            base64_first = self._encode_image_base64(frame_first)
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_first}",
                    "detail": "high"
                }
            })
        except:
            pass

        # Add LAST frame
        try:
            base64_last = self._encode_image_base64(frame_last)
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_last}",
                    "detail": "high"
                }
            })
        except:
            pass

        # Call API
        prompt = self._call_vision_api(
            messages=[{"role": "user", "content": content}],
            max_tokens=500
        )

        if not prompt:
            return None

        # Extract prompt text (remove "PROMPT:" prefix if exists)
        prompt_text = prompt.strip()
        if prompt_text.startswith("PROMPT:"):
            prompt_text = prompt_text[7:].strip()

        scene['sora_prompt'] = prompt_text

        return scene

    # ========== EXPORT ==========

    def _export_results(self) -> str:
        """Export scenes v·ªõi ·∫£nh ƒë·∫ßu + cu·ªëi + prompt"""
        print_progress("ƒêang export k·∫øt qu·∫£...")

        try:
            safe_title = "".join(c for c in self.video_title if c.isalnum() or c in (' ', '_', '-'))[:50]
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_folder = Path(Config.OUTPUT_DIR) / f"{safe_title}_{timestamp}"
            output_folder.mkdir(exist_ok=True, parents=True)

            # Copy images v√† t·∫°o prompt files cho t·ª´ng scene
            for scene in self.scenes:
                scene_id = scene['scene_id']
                scene_folder = output_folder / f"scene_{scene_id:04d}"
                scene_folder.mkdir(exist_ok=True)

                # Copy frame ƒë·∫ßu
                if 'frame_first' in scene and os.path.exists(scene['frame_first']):
                    shutil.copy(
                        scene['frame_first'],
                        scene_folder / f"FIRST_frame.jpg"
                    )

                # Copy frame cu·ªëi
                if 'frame_last' in scene and os.path.exists(scene['frame_last']):
                    shutil.copy(
                        scene['frame_last'],
                        scene_folder / f"LAST_frame.jpg"
                    )

                # Save prompt
                if 'sora_prompt' in scene:
                    prompt_file = scene_folder / "sora_prompt.txt"
                    with open(prompt_file, 'w', encoding='utf-8') as f:
                        f.write(f"Scene {scene_id + 1}\n")
                        f.write(f"Duration: {scene['duration']:.1f}s\n")
                        f.write(f"Time: {scene['start_time']:.1f}s - {scene['end_time']:.1f}s\n")
                        f.write(f"\n{'='*70}\n")
                        f.write(f"SORA 2 PROMPT:\n")
                        f.write(f"{'='*70}\n\n")
                        f.write(scene['sora_prompt'])

            # T·∫°o summary file
            summary_file = output_folder / "00_SUMMARY.txt"
            with open(summary_file, 'w', encoding='utf-8') as f:
                f.write(f"{'='*70}\n")
                f.write(f"YOUTUBE SCENE-BY-SCENE ANALYSIS\n")
                f.write(f"{'='*70}\n\n")
                f.write(f"Video: {self.video_title}\n")
                f.write(f"URL: {self.youtube_url}\n")
                f.write(f"Total Scenes: {len(self.scenes)}\n")
                f.write(f"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"{'='*70}\n")
                f.write(f"SCENE LIST\n")
                f.write(f"{'='*70}\n\n")

                for scene in self.scenes:
                    f.write(f"Scene {scene['scene_id'] + 1}:\n")
                    f.write(f"  - Duration: {scene['duration']:.1f}s\n")
                    f.write(f"  - Time: {scene['start_time']:.1f}s - {scene['end_time']:.1f}s\n")
                    f.write(f"  - Folder: scene_{scene['scene_id']:04d}/\n")
                    if 'sora_prompt' in scene:
                        f.write(f"  - Prompt: {scene['sora_prompt'][:100]}...\n")
                    f.write(f"\n")

            # JSON export
            json_file = output_folder / "scenes_data.json"
            json_data = {
                'video_info': {
                    'title': self.video_title,
                    'url': self.youtube_url,
                    'metadata': self.video_metadata,
                    'total_scenes': len(self.scenes)
                },
                'scenes': [
                    {
                        'scene_id': s['scene_id'],
                        'start_time': s['start_time'],
                        'end_time': s['end_time'],
                        'duration': s['duration'],
                        'sora_prompt': s.get('sora_prompt', ''),
                        'frame_first': f"scene_{s['scene_id']:04d}/FIRST_frame.jpg",
                        'frame_last': f"scene_{s['scene_id']:04d}/LAST_frame.jpg"
                    }
                    for s in self.scenes
                ],
                'export_date': datetime.now().isoformat()
            }

            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)

            print_success(f"ƒê√£ export v√†o: {output_folder}/")
            print_success(f"  - {len(self.scenes)} scene folders (m·ªói folder c√≥: FIRST_frame.jpg + LAST_frame.jpg + sora_prompt.txt)")
            print_success(f"  - 00_SUMMARY.txt (t·ªïng quan)")
            print_success(f"  - scenes_data.json (d·ªØ li·ªáu JSON)")

            return str(output_folder)

        except Exception as e:
            print_error(f"L·ªói export: {e}")
            return ""

    # ========== MAIN PROCESS ==========

    def analyze(self, youtube_url: str) -> Optional[str]:
        """Ph√¢n t√≠ch video v√† t·∫°o prompt cho t·ª´ng scene"""
        self.youtube_url = youtube_url

        print_header("YOUTUBE SCENE-BY-SCENE ANALYZER")
        print("üìπ Xu·∫•t ·∫£nh ƒë·∫ßu + ·∫£nh cu·ªëi cho m·ªói c·∫£nh")
        print("üé¨ T·∫°o prompt cinema chi ti·∫øt cho M·ªñI c·∫£nh\n")

        # Get metadata
        self._get_metadata(youtube_url)

        # Download video
        if not self._download_video(youtube_url):
            return None

        # Detect scenes
        if not self._detect_scenes():
            cleanup_temp()
            return None

        # Extract first & last frames
        if not self._extract_first_last_frames():
            cleanup_temp()
            return None

        # Analyze each scene and generate prompts
        print_header(f"ANALYZING {len(self.scenes)} SCENES")

        for scene in self.scenes:
            result = self._analyze_scene_and_generate_prompt(scene)
            if result:
                print_success(f"Scene {scene['scene_id'] + 1} ‚úì")
            time.sleep(0.5)

        # Export results
        print_header("EXPORTING RESULTS")
        output_path = self._export_results()

        # Cleanup
        cleanup_temp()

        if output_path:
            print_header("‚úì HO√ÄN T·∫§T!")
            print(f"üìÅ K·∫øt qu·∫£: {output_path}/")
            print(f"üìä T·ªïng: {len(self.scenes)} scenes")
            print(f"üé¨ M·ªói scene c√≥: 2 ·∫£nh (ƒë·∫ßu+cu·ªëi) + 1 prompt chi ti·∫øt\n")

        return output_path

# ========== CLI ==========

def main():
    print_header("YOUTUBE SCENE-BY-SCENE ANALYZER")
    print("üé¨ Ph√¢n t√≠ch t·ª´ng c·∫£nh v√† t·∫°o prompt ri√™ng\n")

    url = input("Nh·∫≠p YouTube URL: ").strip()
    if not url:
        print_error("URL kh√¥ng h·ª£p l·ªá")
        return

    load_env_file()
    api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        api_key = input("Nh·∫≠p OpenAI API Key: ").strip()

    if not api_key:
        print_error("C·∫ßn OpenAI API Key")
        return

    try:
        analyzer = SceneBySceneAnalyzer(api_key=api_key)
        result = analyzer.analyze(youtube_url=url)

        if result:
            print_success("‚úì Ph√¢n t√≠ch th√†nh c√¥ng!")
        else:
            print_error("C√≥ l·ªói x·∫£y ra")

    except KeyboardInterrupt:
        print("\n\n‚ö† ƒê√£ d·ª´ng")
        cleanup_temp()
    except Exception as e:
        print_error(f"L·ªói: {e}")
        import traceback
        traceback.print_exc()
        cleanup_temp()

if __name__ == "__main__":
    main()
